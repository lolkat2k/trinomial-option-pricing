\section{Part III: Implementation in CUDA}

\begin{frame}
  \frametitle{Overall implementation strategy}
  %
  \begin{itemize}
    \item Tree construction and option pricing is a two-step
      process:
      %
      \begin{enumerate}
      \item Qs and alphas:

        values at $i$ depends on values at $i-1$
        $\Rightarrow$ forward iteration.
      \item Option prices:

        values at $i$ depends on values at $i+1$
        $\Rightarrow$ backward iteration.
      \end{enumerate}
      %
    \item At each $i$ let each $j$ in tree correspond to one
      thread.
    \item Use 1-dim. iter. space (threads) and stack
      options.
    \item Perform forward- and back iteration using two
      convergence loops -- ensures that dependencies are not
      violated.
    \item Compute interest rates on the fly to minimize
      memory usage.
  \end{itemize}
  %
\end{frame}


%% translating map
\begin{frame}[fragile]
  \frametitle{map f xs ys}
%
Inherently parallel operator, as each thread can apply the same input
function to different data.
%
\begin{lstlisting}
let map_lens =
  map (\m i ->
    if i < optionsInChunk
    then 2 * m + 1
    else 0)
  ms (iota maxOptionsInChunk)
\end{lstlisting}
%
\begin{lstlisting}
if (threadIdx.x < maxOptionsInChunk) {
  if (threadIdx.x < optionsInChunk) {
    map_lens[threadIdx.x] =
            2 * ms[threadIdx.x] + 1;
  } else {
    map_lens[threadIdx.x] = 0;
  }
}
\end{lstlisting}
\end{frame}

%% translating scatter
\begin{frame}[fragile]
  \frametitle{scatter as is vs}
%
Inherently parallel (assume no duplicates in index?) as each thread
handles the write of one value.
%
%
\begin{lstlisting}
let alphass = scatter
  alphass          -- size: n
  alpha_inds       -- size: k
  alpha_vals       -- size: k
\end{lstlisting}
%
\begin{lstlisting}
if (threadIdx.x < optionsInChunk) {
  alphass[alpha_inds[threadIdx.x]] = alpha_vals[threadIdx.x];
}
\end{lstlisting}
\end{frame}

%% memory management
\begin{frame}
  \frametitle{Memory management}
  %
  \begin{itemize}
    \item CUDAs memory hierarchy is split in three parts:
      %
      \begin{itemize}
        \item Per-thread local memory
        \item Per-block shared memory
        \item Global memory
      \end{itemize}
      %
    \item For each thread CUDA provides 8-10 words of shared memory.
    \item We save 9 arrays of size w storing 32bit floats.
    \item Arrays of size w are stored first so that memory is aligned.
  \end{itemize}
\end{frame}
